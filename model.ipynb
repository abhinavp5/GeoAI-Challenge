{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import logging\n",
    "\n",
    "#model building\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "#plotting and evalueation\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_preprocessing(train_path, test_path, plot = True):\n",
    "\n",
    "    #reading data \n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    #Subtracting 1 from each value in target column to scale to 0 or1 \n",
    "    y_train = (train_df[\"TARGET\"]-1).to_numpy()\n",
    "\n",
    "    #Saving the ID values\n",
    "    train_ids = train_df[\"ID\"]\n",
    "    test_ids = test_df[\"ID\"]\n",
    "\n",
    "    #Dropping the ID colmn\n",
    "    train_df = train_df.drop('ID', axis = 1)\n",
    "    test_df = test_df.drop('ID', axis = 1)\n",
    "\n",
    "    #Creating some additional features\n",
    "\n",
    "    #Normalized Difference Vegetation Index\n",
    "    train_df[\"NDVI\"] = (train_df[\"nir_p50\"] - train_df[\"red_p50\"])/ (test_df[\"nir_p50\"] + test_df[\"red_p50\"]) \n",
    "    #Normalized Difference Water Index\n",
    "    train_df[\"NDWI\"] = (train_df[\"nir_p50\"] - train_df[\"swir1_p50\"])/ (test_df[\"nir_p50\"] + test_df[\"swir1_p50\"])\n",
    "\n",
    "    train_df[\"red_green_ratio\"] = train_df[\"red_p50\"]/train_df[\"green_p50\"]\n",
    "    train_df[\"NIR_green_ratio\"] = train_df[\"nir_p50\"]/train_df[\"green_p50\"]\n",
    "\n",
    "    train_df[\"blue_red_ratio\"] = train_df[\"blue_p50\"]/train_df[\"red_p50\"]\n",
    "    train_df[\"swir_ratio\"] = train_df[\"swir1_p50\"]/train_df[\"swir1_p50\"]\n",
    "\n",
    "    train_df[\"VV_VH_ratio\"] = train_df[\"VV_p50\"]/train_df[\"VH_p50\"]\n",
    "\n",
    "\n",
    "    #convering to Numpy arrays\n",
    "    x_train = train_df.to_numpy()\n",
    "    x_test = test_df.to_numpy()\n",
    "\n",
    "    #normalizing the data \n",
    "    x_train = preprocessing.normalize(x_train)\n",
    "    x_test = preprocessing.normalize(x_test)\n",
    "\n",
    "    #Converting to Torch Tensors\n",
    "    x_train = torch.from_numpy(x_train).type(torch.float)\n",
    "    x_test = torch.from_numpy(x_test).type(torch.float)\n",
    "    y_train = torch.from_numpy(y_train).type(torch.float)\n",
    "\n",
    "    return x_train, x_test, y_train, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n",
      "15\n",
      "Index(['ID', 'lon', 'lat', 'blue_p50', 'green_p50', 'nir_p50', 'nira_p50',\n",
      "       're1_p50', 're2_p50', 're3_p50', 'red_p50', 'swir1_p50', 'swir2_p50',\n",
      "       'VV_p50', 'VH_p50'],\n",
      "      dtype='object')\n",
      "Index(['ID  ', 'lon          ', 'lat         ', 'blue_p50 ', 'green_p50 ',\n",
      "       'nir_p50 ', 'nira_p50 ', 're1_p50 ', 're2_p50 ', 're3_p50 ', 'red_p50 ',\n",
      "       'swir1_p50 ', 'swir2_p50 ', 'VV_p50       ', 'VH_p50'],\n",
      "      dtype='object')\n",
      "Index(['ID', 'Lon', 'Lat', 'blue_p50', 'green_p50', 'nir_p50', 'nira_p50',\n",
      "       're1_p50', 're2_p50', 're3_p50', 'red_p50', 'swir1_p50', 'swir2_p50',\n",
      "       'VV_p50', 'VH_p50'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#reading the data\n",
    "kenya_train_df = pd.read_csv(\"geoai-challenge-for-agricultural-plastic-cover-mapping-with-satellite-imagery20240708-24674-1c1nnx3/Kenya_training.csv\")\n",
    "kenya_test_df = pd.read_csv(\"geoai-challenge-for-agricultural-plastic-cover-mapping-with-satellite-imagery20240708-24674-1c1nnx3/Kenya_testing.csv\")\n",
    "\n",
    "spain_train_df = pd.read_csv(\"geoai-challenge-for-agricultural-plastic-cover-mapping-with-satellite-imagery20240708-24674-1c1nnx3/Spain_training.csv\")\n",
    "spain_test_df = pd.read_csv(\"geoai-challenge-for-agricultural-plastic-cover-mapping-with-satellite-imagery20240708-24674-1c1nnx3/Spain_validation.csv\")\n",
    "\n",
    "vietnam_train_df = pd.read_csv(\"geoai-challenge-for-agricultural-plastic-cover-mapping-with-satellite-imagery20240708-24674-1c1nnx3/VNM_training.csv\")\n",
    "vietnam_test_df = pd.read_csv(\"geoai-challenge-for-agricultural-plastic-cover-mapping-with-satellite-imagery20240708-24674-1c1nnx3/VNM_testing.csv\")\n",
    "\n",
    "print(len(kenya_test_df.columns))\n",
    "print(len(spain_test_df.columns))\n",
    "print(len(vietnam_test_df.columns))\n",
    "\n",
    "\n",
    "print((kenya_test_df.columns))\n",
    "print((spain_test_df.columns))\n",
    "print((vietnam_test_df.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the id Variable\n",
    "y_train_kenya = (kenya_train_df[\"TARGET\"]-1).to_numpy()\n",
    "\n",
    "# y_train_vietnam = (vietnam_train_df[\"TARGET\"]-1).to_numpy()\n",
    "# y_test_vietnam = (vietnam_train_df[\"TARGET\"]-1).to_numpy()\n",
    "\n",
    "# y_train_spain = spain_train_df.to_numpy()\n",
    "# y_test_spain = spain_test_df.to_numpy()\n",
    "\n",
    "\n",
    "# #Dropping the id column\n",
    "kenya_train_df = kenya_train_df.drop('ID', axis = 1)\n",
    "kenya_test_df = kenya_test_df.drop('ID', axis = 1)\n",
    "\n",
    "# vietnam_train_df = vietnam_train_df.drop('ID', axis = 1)\n",
    "# vietnam_test_df = vietnam_test_df.drop('ID', axis = 1)\n",
    "\n",
    "# spain_train_df = spain_train_df.drop('ID', axis = 1)\n",
    "# spain_test_df = spain_test_df.drop('ID', axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# #converting to np arrays\n",
    "x_train_kenya = kenya_train_df.to_numpy()\n",
    "x_test_kenya = kenya_test_df.to_numpy()\n",
    "\n",
    "# x_train_vietnam = vietnam_train_df.to_numpy()\n",
    "# x_test_vietnam = vietnam_test_df.to_numpy()\n",
    "\n",
    "# x_train_spain = spain_train_df.to_numpy()\n",
    "# x_test_spain = spain_test_df.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the data \n",
    "x_train_kenya = preprocessing.normalize(x_train_kenya)\n",
    "x_test_kenya = preprocessing.normalize(x_test_kenya)\n",
    "\n",
    "# x_train_vietnam = preprocessing.normalize(x_train_vietnam)\n",
    "# x_test_vietnam = preprocessing.normalize(x_test_vietnam)\n",
    "\n",
    "# x_train_spain = preprocessing.normalize(x_train_spain)\n",
    "# x_test_spain= preprocessing.normalize(x_test_spain)\n",
    "\n",
    "\n",
    "# #Turning the data into torch torch tensors for gpu acceleration\n",
    "x_train_kenya = torch.from_numpy(x_train_kenya).type(torch.float)\n",
    "x_test_kenya = torch.from_numpy(x_test_kenya).type(torch.float)\n",
    "y_train_kenya = torch.from_numpy(y_train_kenya).type(torch.float)\n",
    "\n",
    "# x_train_vietnam = torch.from_numpy(x_train_vietnam).type(torch.float)\n",
    "# x_test_vietnam= torch.from_numpy(x_test_vietnam).type(torch.float)\n",
    "# y_train_vietnam = torch.from_numpy(y_train_vietnam).type(torch.float)\n",
    "# y_test_vietnam = torch.from_numpy(y_test_vietnam).type(torch.float)\n",
    "\n",
    "# x_train_spain = torch.from_numpy(x_train_spain).type(torch.float)\n",
    "# x_test_spain = torch.from_numpy(x_test_spain).type(torch.float)\n",
    "# y_train_spain = torch.from_numpy(y_train_spain).type(torch.float)\n",
    "# y_test_spain = torch.from_numpy(y_test_spain).type(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Custom Dataset\n",
    "class CountryDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.features = x\n",
    "        self.labels = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self. labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending all data framees\n",
    "\n",
    "# Convert DataFrames to tensors before concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instansiating Custom Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_kenya, y_train_kenya,kenya_test_df, kenya_test_ids = pre_preprocessing(train_path = \"geoai-challenge-for-agricultural-plastic-cover-mapping-with-satellite-imagery20240708-24674-1c1nnx3/Kenya_training.csv\",\n",
    "                                                               test_path = \"geoai-challenge-for-agricultural-plastic-cover-mapping-with-satellite-imagery20240708-24674-1c1nnx3/Kenya_testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "#creating custom Datasets\n",
    "train_dataset_kenya = CountryDataset(x_train_kenya, y_train_kenya)\n",
    "\n",
    "\n",
    "\n",
    "#creating custom DataLoaders\n",
    "kenya_train_dl = DataLoader(train_dataset_kenya,batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Module\n",
    "class First_Model(nn.Module,):\n",
    "    def __init__(self):\n",
    "        super(First_Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(22, 128),  # Input size 22features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),   # Output size is 1 for binary classification\n",
    "            nn.Sigmoid()         # Use Sigmoid for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, loss_fn, train_loader, opt, epochs = 1000):\n",
    "\n",
    "    running_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0 \n",
    "        for i, (datapoints, labels) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "\n",
    "            #forward proporgation\n",
    "            outputs = model(datapoints)\n",
    "\n",
    "            print(\"LABELS\", labels.size())\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            print(\"OUTPUS\", outputs.size())\n",
    "            print(\"LABELS\", labels.size())\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            #backpropogation\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "\n",
    "            running_losses.append(loss.item())\n",
    "            epoch_loss+=loss.item()\n",
    "\n",
    "            # if (i+1) %10 == 0:\n",
    "            #     print(f\"epoch number {epoch+1}, loss = {loss.item()}\")\n",
    "    print(\"Training Finished\")\n",
    "    return running_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to Calculate Accuracy of the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, test_loader):\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for datapoints, labels in test_loader:\n",
    "            outputs = model(datapoints)\n",
    "\n",
    "            #values between 0 and 1\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "            #binary classification\n",
    "            predicted = (probabilities > 0.5).float()\n",
    "\n",
    "            #adjusting shape\n",
    "            labels = labels.unsqueeze(1)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(x_train_kenya.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUS torch.Size([32, 1])\n",
      "LABELS torch.Size([32, 1, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32, 1, 14])) must be the same as input size (torch.Size([32, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m loss_function  \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m running_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkenya_train_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m  calculate_accuracy(model, kenya_train_dl)\n",
      "Cell \u001b[0;32mIn[81], line 18\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, loss_fn, train_loader, opt, epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUTPUS\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLABELS\u001b[39m\u001b[38;5;124m\"\u001b[39m, labels\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#backpropogation\u001b[39;00m\n\u001b[1;32m     21\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:3197\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([32, 1, 14])) must be the same as input size (torch.Size([32, 1]))"
     ]
    }
   ],
   "source": [
    "n_features = x_train_kenya.shape[1]\n",
    "model = First_Model()\n",
    "loss_function  = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
    "\n",
    "running_losses = training(model = model, loss_fn = loss_function, train_loader = kenya_train_dl, opt = optimizer)\n",
    "\n",
    "accuracy =  calculate_accuracy(model, kenya_train_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
